# -*- coding: utf-8 -*-
"""Customer/K-Means/Hierarchical Grouping/DBSCAN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/customer-k-means-hierarchical-grouping-dbscan-4f35c926-f224-4ba0-a145-e7529596deb5.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240129/auto/storage/goog4_request%26X-Goog-Date%3D20240129T153949Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4f31f26aa800eed099d3a76b1e0c8da9a1c752d471f906fa73ea65366e0759572a52f8f981510bc163d39e8cca8db810118943a09e0b78b04a43874ec5ed31638ee0d73acf98be0690e96f68be5b3642632d2a76c25f2753366a7be9c676b606894d2c39b320a1dc897cf15a54c9f5ac564db94917aa744f619743da59bd810702755ce217520313bf40215d277b41cb3211d3958a7a77657f3b8560ff748854f576029058ee2994078346b5e6f6bb6fb4cab4c628441e81d46e21540a4c54668954f95da3cdb285381759bc115719fc8f303fe62f3749bf1e23044f65ac2dfb8b5052aea3883ac180bb90821d1d7e64a5f858257c3af1760fda4b90600a529b
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'customer-segmentation:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F841888%2F2811566%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240129%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240129T153949Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8b1ebb48882fb131c6960940b43cf2f3e0dad19db31faafd31ad490306d4be810087672940a5badf9d7ec4697921aaa8209417d754dcdd54e9abbc513ace139312f9cf073c2133e9f9fcf83ec1293a765d80bb5f0e0e47a903e21c023aaaa221858f3e482152a723c06f674e553c6e3ade03a108a5e7c207bd64824e4c142e869c2a4ae4e3b53757054df32764e32381c21d4672472a4c5170e59a9fa35b31f35e50b4479518fc4c3a9afeec18165f4b9af162cfdfb3c983796e445b0b205d0c37807a1f21ffd26a64d4f4d265d3dfc004863ebb5b50074aa1b9da61ea2a67b105b4061c064954734de00f784334269f3ef46e214d7b1c36636d5392ef2b8b2c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

"""# **Context**

An automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4, and P5). After intensive market research, they’ve deduced that the behavior of the new market is similar to their existing market.


In their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for a different segment of customers. This strategy has work e exceptionally well for them. They plan to use the same strategy for the new markets and have identified 2627 new potential customers.


You are required to help the manager to predict the right group of the new customers.


# **Content**
* Variable	- Definition
* ID	- Unique ID
* Gender	- Gender of the customer
* Ever_Married - 	Marital status of the customer
* Age	- Age of the customer
* Graduated -	Is the customer a graduate?
* Profession -	Profession of the customer
* Work_Experience -	Work Experience in years
* Spending_Score	- Spending score of the customer
* Family_Size	- Number of family members for the customer (including the customer)
* Var_1 -	Anonymised Category for the customer
* Segmentation -	(target) Customer Segment of the customer

Acknowledgements
This dataset was acquired from the Analytics Vidhya hackathon.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
# %matplotlib inline
import pandas as pd
import seaborn as sns
import plotly.express as px
import plotly.graph_objs as go
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import  LabelEncoder
from sklearn.preprocessing import StandardScaler
from plotly.offline import init_notebook_mode,iplot

"""# **Training Data**"""

df_train = pd.read_csv('../input/customer-segmentation/Train.csv')
df_train

"""# **Clearing the Data**"""

df_train.isna().sum(axis=0)

"""There is a lot of null data in the training base, if we delete it it will decrease the database a lot.

If it was a company and we had contact with the database supplier, we would try to fill in the missing data.

In order not to lag the database too much, let's fill in the average of each missing value.
"""

sns.heatmap(df_train.isnull());

df_train.fillna(df_train.mean(), inplace = True)

"""There are some categorical data that could not be filled with the mean value, so we will delete them, so we try to delete as little as possible in order to keep the database."""

df_train.dropna(axis=0, inplace=True)

df_train.isna().sum(axis=0)

df_train = df_train.drop(columns=["ID"])

"""# **Exploring the Data**"""

temp = df_train.describe()
temp.style.background_gradient(cmap='Oranges')

g1 = [go.Box(y=df_train.Work_Experience,name="Work_Experience",marker=dict(color="rgba(51,0,0,0.9)"),hoverinfo="name+y")]
g2 = [go.Box(y=df_train.Family_Size,name="Family_Size",marker=dict(color="rgba(0,102,102,0.9)"),hoverinfo="name+y")]
layout2 = go.Layout(title="Work Experience | Family Size",yaxis=dict(range=[0,13]))
fig2 = go.Figure(data=g1+g2,layout=layout2)
iplot(fig2)

grafico = px.box(df_train, y='Age')
grafico.show()

fig2 = px.histogram(df_train,x='Age',color='Age',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Gender',color='Gender',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Ever_Married',color='Ever_Married',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Graduated',color='Graduated',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Profession',color='Profession',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Work_Experience',color='Work_Experience',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Spending_Score',color='Spending_Score',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Family_Size',color='Family_Size',template='plotly_dark')
fig2.show()

fig2 = px.histogram(df_train,x='Var_1',color='Var_1',template='plotly_dark')
fig2.show()

"""**Through the graphics it was possible to analyze all the columns and have details of each one of them.**

# **Processing**
"""

mk = LabelEncoder()
df_train['Gender'] = mk.fit_transform(df_train['Gender'])
df_train['Ever_Married'] = mk.fit_transform(df_train['Ever_Married'])
df_train['Graduated'] = mk.fit_transform(df_train['Graduated'])
df_train['Spending_Score'] = mk.fit_transform(df_train['Spending_Score'])
df_train['Var_1'] = mk.fit_transform(df_train['Var_1'])
df_train['Profession'] = mk.fit_transform(df_train['Profession'])
df_train['Family_Size'] = mk.fit_transform(df_train['Family_Size'])
df_train['Work_Experience'] = mk.fit_transform(df_train['Work_Experience'])
df_train['Segmentation'] = mk.fit_transform(df_train['Segmentation'])

df_train

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df_train)

type(df_scaled)

min(df_scaled[0]), max(df_scaled[0])

df_scaled

wcss_1 = []
range_values = range(1, 10)
for i in range_values:
  kmeans = KMeans(n_clusters=i)
  kmeans.fit(df_scaled)
  wcss_1.append(kmeans.inertia_)

print(wcss_1)

grafico = px.line(x = range(1,10), y = wcss_1)
plt.plot(wcss_1, '-o',)
grafico.show()

kmeans = KMeans(n_clusters=5)
kmeans.fit(df_scaled)
labels = kmeans.labels_

labels, len(labels)

np.unique(labels, return_counts=True)

cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [df_train.columns])
cluster_centers

cluster_centers = scaler.inverse_transform(cluster_centers)
cluster_centers = pd.DataFrame(data = cluster_centers, columns = [df_train.columns])
cluster_centers

df_mk_cluster = pd.concat([df_train, pd.DataFrame({'cluster': labels})], axis = 1)
df_mk_cluster.head()

sns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'black', 'axes.grid' : False, 'font.family': 'Ubuntu'})

for i in df_mk_cluster:
    g = sns.FacetGrid(df_mk_cluster, col = "cluster", hue = "cluster", palette = "Set2")
    g.map(plt.hist, i, bins=10, ec="k")
    g.set_xticklabels(rotation=30, color = 'white')
    g.set_yticklabels(color = 'white')
    g.set_xlabels(size=15, color = 'white')
    g.set_titles(size=15, color = '#FFC300', fontweight="bold")
    g.fig.set_figheight(5);

import matplotlib as mpl

clusters_count = df_mk_cluster['cluster'].value_counts()
clusters_count = clusters_count.to_frame().reset_index()
clusters_count.columns = ['clusters', 'count']
clusters_count = clusters_count.sort_values('clusters', ascending = True)

labels = [
        "B",
        "A",
        "D",
        "E",
        "C"
        ]

plt.figure(figsize=(15,9))
mpl.rcParams['font.size'] = 17
colors = sns.color_palette('Set2')[0:5]
plt.pie(clusters_count['count'],
        explode=(0.05, 0.05, 0.05, 0.05, 0.05),
        labels = labels,
        colors= colors,
        autopct='%1.1f%%',
        textprops = dict(color ="white", fontsize=19),
        counterclock = False,
        startangle=180,
        wedgeprops={"edgecolor":"gray",'linewidth':1}
        )

plt.axis('equal')
plt.text(-0.8, 1.2, "Clusters", size=30, color="#FFC300", fontweight="bold")
plt.text(-1.8, 1.2, "Distribution", size=30, color="white")
plt.show();

"""# **Hierarchical Grouping**"""

df_scaled

import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage

dendrograma = dendrogram(linkage(df_scaled, method='ward'))
plt.title('Dendrograma')
plt.xlabel('X')
plt.ylabel('Y');

from sklearn.cluster import AgglomerativeClustering

hc_g = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage = 'ward')
rotulos = hc_g.fit_predict(df_scaled)

rotulos

grafico = px.scatter(x = df_scaled[:,0], y = df_scaled[:,1], color = rotulos)
grafico.show()

"""# **DBSCAN**"""

df_scaled

from sklearn.cluster import DBSCAN

dbscan_g = DBSCAN(eps = 0.95, min_samples=2)
dbscan_g.fit(df_train)

rotulos = dbscan_g.labels_
rotulos

grafico = px.scatter(x = df_scaled[:,0], y = df_scaled[:,1], color = rotulos)
grafico.show()

"""# **K-means x Hierarchical x DBSCAN**"""

from sklearn import datasets

X_random, y_random = datasets.make_moons(n_samples=1500, noise = 0.09)

X_random

"""# Data Visualization"""

grafico = px.scatter(x = X_random[:,0], y = X_random[:,1])
grafico.show()

"""#  KMeans"""

kmeans = KMeans(n_clusters=2)
rotulos = kmeans.fit_predict(X_random)
grafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)
grafico.show()

"""**As we can see Kmena was not able to do an efficient separation of the data, it would not be efficient for sorting this data;**

# Agglomerative Clustering
"""

hc = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')
rotulos = hc.fit_predict(X_random)
grafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)
grafico.show()

"""**It wasn't very efficient in the separation either.**

# DBSCAN
"""

dbscan = DBSCAN(eps=0.1)
rotulos = dbscan.fit_predict(X_random)
grafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)
grafico.show()

"""There are some groups that he could not associate, which are dark blue in color;

We can verify that the DBDCAN algorithm did better in the grouping, we defined the radio 0.1, which helps a lot in the separation;

We have a small database however, we could use the two previous more traditional algorithms for the type of study in this database,

In general DBDCAN presents better results than k-means and also faster, what improves the algorithm a lot is the threshold distance, it ends up being more interesting when we have more complex problems and with a much larger database.

# **If you find this notebook useful, support with an upvote** 👍
"""